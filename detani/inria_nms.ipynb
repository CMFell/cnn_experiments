{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torchvision import transforms as transforms\n",
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "from window.utils.tiling import create_tile_list\n",
    "from window.models.yolo_for_inference import YoloClass\n",
    "from window.utils.truths import windows_truth\n",
    "from window.utils.inference_windows import process_annotation_df_negative_inference, create_windows_from_yolo, windows_to_whole_im, xc_to_xmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_whole_image_dir_inria = \"/data/old_home_dir/ChrissyF/INRIA/Valid/whole_images/\"\n",
    "truth_file_inria = \"/data/old_home_dir/ChrissyF/INRIA/00INRIA_bboxes_valid_yolo.csv\"\n",
    "\n",
    "valid_whole_image_dir_gfrc = \"/data/old_home_dir/ChrissyF/GFRC/Valid/whole_images_all/\"\n",
    "truth_file_gfrc = \"/data/old_home_dir/ChrissyF/GFRC/yolo_valid_GFRC_bboxes.csv\"\n",
    "\n",
    "valid_whole_image_dir_vedai = \"/data/old_home_dir/ChrissyF/VEDAI/Valid/whole_images/\"\n",
    "truth_file_vedai = \"/data/old_home_dir/ChrissyF/VEDAI/Valid/00VEDAI_test_bboxes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_dir_inria = \"/home/cmf21/pytorch_save/INRIA/rgb_baseline/\"\n",
    "output_base_dir_vedai = \"/home/cmf21/pytorch_save/VEDAI/Bin/rgb_baseline/\"\n",
    "output_base_dir_gfrc = \"/home/cmf21/pytorch_save/GFRC/Bin/rgb_baseline2/\"\n",
    "output_base_dir_gfrc_hnm = \"/home/cmf21/pytorch_save/GFRC/Bin/rgb_baseline2_hnm/\"\n",
    "output_base_dir_inria_grey = \"/home/cmf21/pytorch_save/INRIA/grey_baseline/\"\n",
    "output_base_dir_vedai_grey = \"/home/cmf21/pytorch_save/VEDAI/Bin/grey_baseline/\"\n",
    "output_base_dir_gfrc_grey = \"/home/cmf21/pytorch_save/GFRC/Bin/grey_baseline/\"\n",
    "output_base_dir_vedai_multi = \"/home/cmf21/pytorch_save/VEDAI/Multi/rgb_baseline/\"\n",
    "output_base_dir_gfrc_multi = \"/home/cmf21/pytorch_save/GFRC/Multi/rgb_baseline/\"\n",
    "output_base_dir_vedai_multi_grey = \"/home/cmf21/pytorch_save/VEDAI/Multi/grey_baseline/\"\n",
    "output_base_dir_gfrc_multi_grey = \"/home/cmf21/pytorch_save/GFRC/Multi/grey_baseline/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'gfrc_hnm'\n",
    "predict_results = True\n",
    "\n",
    "if experiment == 'inria_bin':\n",
    "    output_base_dir = output_base_dir_inria\n",
    "    valid_whole_image_dir = valid_whole_image_dir_inria\n",
    "    truth_file = truth_file_inria\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out236_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_236.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 1\n",
    "    dataset = 'inria'\n",
    "elif experiment == 'inria_bin_grey':\n",
    "    output_base_dir = output_base_dir_inria_grey\n",
    "    valid_whole_image_dir = valid_whole_image_dir_inria\n",
    "    truth_file = truth_file_inria\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out63_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_63.pt\"\n",
    "    channels_in = 1\n",
    "    nclazz = 1\n",
    "    dataset = 'inria'\n",
    "elif experiment == 'vedai_bin':\n",
    "    output_base_dir = output_base_dir_vedai\n",
    "    valid_whole_image_dir = valid_whole_image_dir_vedai\n",
    "    truth_file = truth_file_vedai\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out922_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_922.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 1\n",
    "    dataset = 'vedai'\n",
    "elif experiment == 'vedai_bin_grey':\n",
    "    output_base_dir = output_base_dir_vedai_grey\n",
    "    valid_whole_image_dir = valid_whole_image_dir_vedai\n",
    "    truth_file = truth_file_vedai\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out484_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_484.pt\"\n",
    "    channels_in = 1\n",
    "    nclazz = 1\n",
    "    dataset = 'vedai'\n",
    "elif experiment == 'vedai_multi':\n",
    "    output_base_dir = output_base_dir_vedai_multi\n",
    "    valid_whole_image_dir = valid_whole_image_dir_vedai\n",
    "    truth_file = truth_file_vedai\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out111_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_111.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 9\n",
    "    dataset = 'vedai'\n",
    "elif experiment == 'vedai_multi_grey':\n",
    "    output_base_dir = output_base_dir_vedai_multi_grey\n",
    "    valid_whole_image_dir = valid_whole_image_dir_vedai\n",
    "    truth_file = truth_file_vedai\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out452_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_452.pt\"\n",
    "    channels_in = 1\n",
    "    nclazz = 9\n",
    "    dataset = 'vedai'\n",
    "elif experiment == 'gfrc_bin':\n",
    "    output_base_dir = output_base_dir_gfrc\n",
    "    valid_whole_image_dir = valid_whole_image_dir_gfrc\n",
    "    truth_file = truth_file_gfrc\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out163_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_163.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 1\n",
    "    dataset = 'gfrc'\n",
    "elif experiment == 'gfrc_bin_grey':\n",
    "    output_base_dir = output_base_dir_gfrc_grey\n",
    "    valid_whole_image_dir = valid_whole_image_dir_gfrc\n",
    "    truth_file = truth_file_gfrc\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out265_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_265.pt\"\n",
    "    channels_in = 1\n",
    "    nclazz = 1\n",
    "    dataset = 'gfrc'\n",
    "elif experiment == 'gfrc_multi':\n",
    "    output_base_dir = output_base_dir_gfrc_multi\n",
    "    valid_whole_image_dir = valid_whole_image_dir_gfrc\n",
    "    truth_file = truth_file_gfrc\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out189_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_189.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 6\n",
    "    dataset = 'gfrc'\n",
    "elif experiment == 'gfrc_multi_grey':\n",
    "    output_base_dir = output_base_dir_gfrc_multi_grey\n",
    "    valid_whole_image_dir = valid_whole_image_dir_gfrc\n",
    "    truth_file = truth_file_gfrc\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out274_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_274.pt\"\n",
    "    channels_in = 1\n",
    "    nclazz = 6\n",
    "    dataset = 'gfrc'\n",
    "elif experiment == 'gfrc_hnm':\n",
    "    output_base_dir = output_base_dir_gfrc_hnm\n",
    "    valid_whole_image_dir = valid_whole_image_dir_gfrc\n",
    "    truth_file = truth_file_gfrc\n",
    "    predicted_object_loc = output_base_dir + 'boxes_out97_full.csv'\n",
    "    saveweightspath = output_base_dir + \"testing_save_97.pt\"\n",
    "    channels_in = 3\n",
    "    nclazz = 2\n",
    "    dataset = 'gfrc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = output_base_dir + \"valid_images_out/\"\n",
    "output_csv = output_dir + \"valid_results/results_matched_nms.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image files\n",
    "image_files_jpg = list(Path(valid_whole_image_dir).glob(\"*.jpg\"))\n",
    "image_files_png = list(Path(valid_whole_image_dir).glob(\"*.png\"))\n",
    "image_files = image_files_jpg + image_files_png\n",
    "image_files = [img.name for img in image_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process truth files\n",
    "truths = pd.read_csv(truth_file)\n",
    "if dataset == 'gfrc':\n",
    "    truths.loc[:, 'filename'] = [strin.replace('/', '_') for strin in truths.file_loc]\n",
    "    truths = windows_truth(truths)\n",
    "else:\n",
    "    truths['filename'] = truths['file_loc']\n",
    "    truths['xmn'] = truths['xmin']\n",
    "    truths['xmx'] = truths['xmax']\n",
    "    truths['ymn'] = truths['ymin']\n",
    "    truths['ymx'] = truths['ymax']\n",
    "if dataset == 'vedai':\n",
    "    truths['oc'] = truths['class_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_filename(str_in):\n",
    "    file_nm = str_in[:-4]\n",
    "    file_splt = file_nm.split('_')\n",
    "    file_out = file_splt[0] + '_' + file_splt[1] + '.jpg'\n",
    "    tile_out = file_splt[2]\n",
    "    return file_out, tile_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yolo model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_results:\n",
    "    if dataset == 'inria':\n",
    "        yolo_model = YoloClass(wtpath=saveweightspath, channels=channels_in, img_w=640, img_h=480, nclazz=nclazz)\n",
    "    elif dataset == 'vedai':\n",
    "        yolo_model = YoloClass(wtpath=saveweightspath, channels=channels_in, img_w=1024, img_h=1024, nclazz=nclazz)\n",
    "    else:\n",
    "        yolo_model = YoloClass(wtpath=saveweightspath, channels=channels_in, nclazz=nclazz)\n",
    "        \n",
    "    windows_whole = pd.DataFrame(columns=['xc', 'yc', 'wid', 'hei', 'conf', 'class', 'xmn', 'xmx', 'ymn', 'ymx', 'filename'])\n",
    "\n",
    "    for fl in image_files:\n",
    "        # Per image\n",
    "        \n",
    "        if channels_in == 3:\n",
    "            #whole_im = cv2.imread(valid_whole_image_dir + fl)\n",
    "            #whole_im = cv2.cvtColor(whole_im, cv2.COLOR_BGR2RGB)\n",
    "            whole_im = io.imread(valid_whole_image_dir + fl, as_gray=False)\n",
    "        else:\n",
    "            #whole_im = cv2.imread(valid_whole_image_dir + fl, cv2.IMREAD_GRAYSCALE)\n",
    "            #whole_im = np.array(whole_im / 255, dtype=np.float32)\n",
    "            whole_im = io.imread(valid_whole_image_dir + fl, as_gray=True)\n",
    "            whole_im = np.array(np.divide(whole_im, 255.0), dtype=np.float32)\n",
    "\n",
    "        orig_im_size = whole_im.shape\n",
    "        \n",
    "        if dataset == 'inria':\n",
    "            whole_im = cv2.resize(whole_im, (640, 480))\n",
    "\n",
    "        # create tiles\n",
    "        if dataset == 'gfrc':\n",
    "            tilez = create_tile_list(whole_im)\n",
    "        else:\n",
    "            tilez = [whole_im]\n",
    "\n",
    "        # get predictions from yolo\n",
    "        boxes_whole_im = yolo_model.inference_on_image(tilez, 0.005)\n",
    "\n",
    "        if dataset == 'gfrc':\n",
    "            windows_whole_im = windows_to_whole_im(boxes_whole_im)\n",
    "        else:\n",
    "            windows_whole_im = xc_to_xmn(boxes_whole_im, orig_im_size[1], orig_im_size[0])\n",
    "        \n",
    "        windows_whole_im['filename'] = fl\n",
    "\n",
    "        windows_whole = pd.concat((windows_whole, windows_whole_im), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predict_results:\n",
    "    # read in results\n",
    "    predicted_objects = pd.read_csv(predicted_object_loc)\n",
    "    file_locs = predicted_objects.file\n",
    "    file_locs = [Path(fl).name for fl in file_locs]\n",
    "    if dataset == 'gfrc':\n",
    "        filenames = [split_filename(fl)[0] for fl in file_locs]\n",
    "        tiles = [split_filename(fl)[1] for fl in file_locs]\n",
    "        predicted_objects['filename'] = filenames\n",
    "        predicted_objects['tile'] = tiles\n",
    "        windows_whole = windows_to_whole_im(predicted_objects)\n",
    "    elif dataset == 'inria':\n",
    "        predicted_objects['filename'] = file_locs\n",
    "        windows_whole = pd.DataFrame(columns=['xc', 'yc', 'wid', 'hei', 'file', 'conf', 'class', 'tp', 'filename', 'xmn', 'xmx', 'ymn', 'ymx'])\n",
    "        for idx, fl in enumerate(image_files):\n",
    "            predicted_obs_im = predicted_objects[predicted_objects.filename == fl]\n",
    "            img_in = cv2.imread(valid_whole_image_dir + fl)\n",
    "            windows_im = xc_to_xmn(predicted_obs_im, img_in.shape[1], img_in.shape[0])\n",
    "            windows_whole = pd.concat((windows_whole, windows_im), axis=0)\n",
    "    elif dataset == 'vedai':\n",
    "        predicted_objects['filename'] = file_locs\n",
    "        windows_whole = xc_to_xmn(predicted_objects, 1024, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-roommate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(windows_whole.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_filename = experiment + '_windows.csv'\n",
    "output_dir = \"/home/cmf21/pytorch_save/output_for_draft/\"\n",
    "windows_whole.to_csv(output_dir + windows_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-helena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-israeli",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-musical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-convergence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-reminder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-market",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-investing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-atlas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-seven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_df = results_all_ims[results_all_ims.confmat == 'FN']\n",
    "# [0, 1856, 3712, 5504] [1856, 3712, 5568, 7360] [0, 1248, 2496, 3664] [1248, 2496, 3744, 4912]\n",
    "def is_tile_edge(row_in):\n",
    "    xmn = row_in.xmn - 10\n",
    "    xmx = row_in.xmx + 10\n",
    "    ymn = row_in.ymn - 10\n",
    "    ymx = row_in.ymx + 10\n",
    "    \n",
    "    x1 = xmn < 0 < xmx\n",
    "    x2 = xmn < 1856 < xmx\n",
    "    x3 = xmn < 3712 < xmx\n",
    "    x4 = xmn < 5504 < xmx\n",
    "    x5 = xmn < 5568 < xmx\n",
    "    x6 = xmn < 7360 < xmx\n",
    "    y1 = ymn < 0 < ymx\n",
    "    y2 = ymn < 1248 < ymx\n",
    "    y3 = ymn < 2496 < ymx\n",
    "    y4 = ymn < 3664 < ymx\n",
    "    y5 = ymn < 3744 < ymx\n",
    "    y6 = ymn < 4912 < ymx\n",
    "    edge = x1 | x2 | x3 | x4 | x5 | x6 | y1 | y2 | y3 | y4 | y5 | y6\n",
    "    \n",
    "    return edge\n",
    "\n",
    "fn_edge = []\n",
    "for nn in range(fn_df.shape[0]):\n",
    "    fn_edge.append(is_tile_edge(fn_df.iloc[nn, :]))\n",
    "    \n",
    "print(np.sum(fn_edge)/fn_df.shape[0])\n",
    "\n",
    "# calculate percentage of image that is close to edge of tile\n",
    "(600 * 4912 + 600 * 7360 - 600 * 600) / (4912 * 7360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-archive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-bracket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-toilet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-cancer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
