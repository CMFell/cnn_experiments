{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-penguin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_whole_image_dir = \"/data/old_home_dir/ChrissyF/GFRC/Valid/whole_images_all/\"\n",
    "valid_test_img = \"Z107_Img11169.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in list of truths\n",
    "truth_file = \"/data/old_home_dir/ChrissyF/GFRC/yolo_valid_GFRC_bboxes.csv\"\n",
    "truths = pd.read_csv(truth_file)\n",
    "truths['filename'] = [strin.replace('/', '_') for strin in truths.file_loc]\n",
    "truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "truths_im = truths[truths.filename == valid_test_img]\n",
    "truths_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in image\n",
    "whole_im = cv2.imread(valid_whole_image_dir + valid_test_img)\n",
    "whole_im = cv2.cvtColor(whole_im, cv2.COLOR_BGR2RGB)\n",
    "whole_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into tiles for yolo and keep list of split locations\n",
    "\n",
    "def split_locations_array():\n",
    "    # create list of tile splits\n",
    "    gfrccolst = [0, 1856, 3712, 5504]\n",
    "    gfrccolst = np.reshape(np.tile(gfrccolst, 4), (np.square(4), 1))\n",
    "    gfrccoled = [1856, 3712, 5568, 7360]\n",
    "    gfrccoled = np.reshape(np.tile(gfrccoled, 4), (np.square(4), 1))\n",
    "\n",
    "    gfrcrowst = [0, 1248, 2496, 3664]\n",
    "    gfrcrowst = np.reshape(np.repeat(gfrcrowst, 4), (np.square(4), 1))\n",
    "    gfrcrowed = [1248, 2496, 3744, 4912]\n",
    "    gfrcrowed = np.reshape(np.repeat(gfrcrowed, 4), (np.square(4), 1))\n",
    "\n",
    "    gfrcwindz = np.hstack((gfrcrowst, gfrccolst, gfrcrowed, gfrccoled))\n",
    "    gfrcwindz = np.array(gfrcwindz, dtype=np.int)\n",
    "    \n",
    "    return gfrcwindz\n",
    "\n",
    "def create_tile_list(whole_im):\n",
    "    gfrcwindz = split_locations_array()\n",
    "    \n",
    "    # split image into tiles\n",
    "    im_tiles = []\n",
    "    for tl in gfrcwindz:\n",
    "        tile = whole_im[tl[0]:tl[2], tl[1]:tl[3]]\n",
    "        im_tiles.append(tile)\n",
    "\n",
    "    return im_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilez = create_tile_list(whole_im)\n",
    "    \n",
    "from PIL import Image\n",
    "Image.fromarray(tilez[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on each tile using yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from cnn_experiments.yolo.trch_weights import get_weights\n",
    "from cnn_experiments.yolo.trch_yolonet import YoloNetOrig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_output_to_box_test(y_pred, conf_threshold):    \n",
    "    n_bat = y_pred.shape[0]\n",
    "    # n_bat = int(dict_in['batch_size'])\n",
    "    boxsx = y_pred.shape[2]\n",
    "    # boxsx = int(dict_in['boxs_x'])\n",
    "    boxsy = y_pred.shape[1]\n",
    "    # boxsy = int(dict_in['boxs_y'])\n",
    "    # anchors = dict_in['anchors']\n",
    "    nanchors = anchors.shape[0]\n",
    "    num_out = int(y_pred.shape[3] / nanchors)\n",
    "    n_classes = num_out - 5\n",
    "    # n_classes = int(dict_in['n_classes'])\n",
    "    # num_out = 5 + n_classes\n",
    "    # thresh = dict_in['threshold']\n",
    "    # size of all boxes anchors and data\n",
    "    size1 = [n_bat, boxsy, boxsx, nanchors, num_out]\n",
    "    # size of all boxes and anchors\n",
    "    size2 = [n_bat, boxsy, boxsx, nanchors]\n",
    "    # number of boxes in each direction used for calculations rather than sizing so x first\n",
    "    size3 = [boxsx, boxsy]\n",
    "    # get top left position of cells\n",
    "    rowz = np.arange(boxsy)\n",
    "    colz = np.arange(boxsx)\n",
    "    # rowno = np.reshape(np.repeat(np.repeat(rowz, boxsx * nanchors), n_bat), (n_bat, boxsy, boxsx, nanchors))\n",
    "    rowno = np.expand_dims(np.expand_dims(np.reshape(np.repeat(rowz, boxsx), (boxsy, boxsx)), axis=0), axis=3)\n",
    "    # colno = np.reshape(np.repeat(np.tile(np.repeat(colz, nanchors), boxsy), n_bat), (n_bat, boxsy, boxsx, nanchors))\n",
    "    colno = np.expand_dims(np.expand_dims(np.reshape(np.tile(colz, boxsy), (boxsy, boxsx)), axis=0), axis=3)\n",
    "    tl_cell = np.stack((colno, rowno), axis=4)\n",
    "    # restructure net_output\n",
    "    y_pred = np.reshape(y_pred, size1)\n",
    "\n",
    "    # get confidences centres sizes and class predictions from from net_output\n",
    "    confs_cnn = expit(np.reshape(y_pred[:, :, :, :, 4], size2))\n",
    "    cent_cnn = expit(y_pred[:, :, :, :, 0:2])\n",
    "    # cent_cnn_in = cent_cnn\n",
    "    # add to cent_cnn so is position in whole image\n",
    "    cent_cnn = np.add(cent_cnn, tl_cell)\n",
    "    # divide so position is relative to whole image\n",
    "    cent_cnn = np.divide(cent_cnn, size3)\n",
    "\n",
    "    size_cnn = y_pred[:, :, :, :, 2:4]\n",
    "    # size is to power of prediction\n",
    "    size_cnn = np.exp(size_cnn)\n",
    "    # keep for loss\n",
    "    # size_cnn_in = size_cnn\n",
    "    # adjust so size is relative to anchors\n",
    "    size_cnn = np.multiply(size_cnn, anchors)\n",
    "    # adjust so size is relative to whole image\n",
    "    size_cnn = np.divide(size_cnn, size3)\n",
    "    class_cnn = expit(y_pred[:, :, :, :, 5:])\n",
    "    \n",
    "    for img in range(n_bat):\n",
    "        box_img = pd.DataFrame(columns=['xc', 'yc', 'wid', 'hei', 'conf', 'class'])\n",
    "        for yc in range(boxsy):\n",
    "            for xc in range(boxsx):\n",
    "                for ab in range(nanchors):\n",
    "                    if confs_cnn[img, yc, xc, ab] > conf_threshold:\n",
    "                        # scores_out.append(confs_cnn[img, yc, xc, ab])\n",
    "                        class_out = np.argmax(class_cnn[img, yc, xc, ab, :])\n",
    "                        # classes_out.append(class_out)\n",
    "                        detect_deets = pd.DataFrame(\n",
    "                            [[\n",
    "                                cent_cnn[img, yc, xc, ab, 0],\n",
    "                                cent_cnn[img, yc, xc, ab, 1],\n",
    "                                size_cnn[img, yc, xc, ab, 0],\n",
    "                                size_cnn[img, yc, xc, ab, 1],\n",
    "                                confs_cnn[img, yc, xc, ab],\n",
    "                                class_out\n",
    "                            ]],\n",
    "                            columns=['xc', 'yc', 'wid', 'hei', 'conf', 'class']\n",
    "                        )\n",
    "                        box_img = box_img.append(detect_deets)\n",
    "    \n",
    "    return box_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w = 1856\n",
    "img_h = 1248\n",
    "max_annotations = 14\n",
    "anchors = [[2.387088, 2.985595], [1.540179, 1.654902], [3.961755, 3.936809], [2.681468, 1.803889], [5.319540, 6.116692]]\n",
    "nclazz = 1\n",
    "box_size = [32, 32]\n",
    "weightspath = \"/data/old_home_dir/ChrissyF/GFRC/yolov2.weights\"\n",
    "saveweightspath = \"/home/cmf21/pytorch_save/GFRC/Bin/rgb_baseline2/testing_save_163.pt\"\n",
    "lambda_c = 5.0\n",
    "lambda_no = 0.5\n",
    "lambda_cl = 1\n",
    "lambda_cf = 1\n",
    "n_box = 5\n",
    "out_len = 5 + nclazz\n",
    "fin_size = n_box * out_len\n",
    "grid_w = int(img_w / box_size[1])\n",
    "grid_h = int(img_h / box_size[0])\n",
    "input_vec = [grid_w, grid_h, n_box, out_len]\n",
    "anchors = np.array(anchors)\n",
    "channels_in = 3\n",
    "conf_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "layerlist = get_weights(weightspath)\n",
    "net = YoloNetOrig(layerlist, fin_size, channels_in)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(saveweightspath))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class TileImageTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tiles_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.tiles_list = tiles_list\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.tiles_list[idx]\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tile_dataset = TileImageTestDataset(tilez)\n",
    "tileloader = DataLoader(tile_dataset, batch_size=1, shuffle=False)\n",
    "boxes_whole_im = pd.DataFrame(columns=['xc', 'yc', 'wid', 'hei', 'conf', 'class', 'tile'])\n",
    "for idx, tile in enumerate(tileloader):\n",
    "    tile = tile.to(device)\n",
    "    y_pred = net(tile)\n",
    "    y_pred_np = y_pred.data.cpu().numpy()\n",
    "    boxes_tile = yolo_output_to_box_test(y_pred_np, conf_threshold)\n",
    "    boxes_tile['tile'] = idx\n",
    "    boxes_whole_im = pd.concat((boxes_whole_im, boxes_tile), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation_df_negative(detections_fil, img_size):\n",
    "    pix_width, pix_height = img_size\n",
    "    detections_fil['wid_pixels'] = np.array(np.multiply(detections_fil['wid'], pix_width), dtype=np.int)\n",
    "    detections_fil['hei_pixels'] = np.array(np.multiply(detections_fil['hei'], pix_height), dtype=np.int)\n",
    "    detections_fil['square_size'] = np.maximum(detections_fil['wid_pixels'], detections_fil['hei_pixels'])\n",
    "    detections_fil['xc_pix'] = np.array(np.multiply(detections_fil['xc'], pix_width), dtype=np.int)\n",
    "    detections_fil['yc_pix'] = np.array(np.multiply(detections_fil['yc'], pix_height), dtype=np.int)\n",
    "    detections_fil['xmin'] = np.array(np.minimum(np.maximum(np.subtract(detections_fil['xc_pix'], np.divide(detections_fil['square_size'], 2)), 0), np.subtract(pix_width, detections_fil['square_size'])), dtype=np.int)\n",
    "    detections_fil['ymin'] = np.array(np.minimum(np.maximum(np.subtract(detections_fil['yc_pix'], np.divide(detections_fil['square_size'], 2)), 0), np.subtract(pix_height, detections_fil['square_size'])), dtype=np.int)\n",
    "    detections_fil['xmax'] = np.add(detections_fil['xmin'], detections_fil['square_size'])\n",
    "    detections_fil['ymax'] = np.add(detections_fil['ymin'], detections_fil['square_size'])\n",
    "    detections_fil = detections_fil.reset_index()\n",
    "    return detections_fil\n",
    "\n",
    "def create_windows_from_yolo(windows_in, tile_list):\n",
    "    windowz_out = []\n",
    "    for index, row in windows_in.iterrows():\n",
    "        tile_for_row = tile_list[row.tile]\n",
    "        row_array = tile_for_row[row.ymin:row.ymax, row.xmin:row.xmax]\n",
    "        row_pil = Image.fromarray(row_array)\n",
    "        windowz_out.append(row_pil)\n",
    "    return windowz_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output positions to windows\n",
    "\n",
    "windows_whole_im = process_annotation_df_negative(boxes_whole_im, [img_w, img_h])\n",
    "\n",
    "windows_list = create_windows_from_yolo(windows_whole_im, tilez)\n",
    "\n",
    "windows_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# predict on windows\n",
    "class BinaryWindowClassifier(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = inception_v3(num_classes=2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output, aux1 = self.model(x)\n",
    "        pred = torch.log_softmax(output, dim=1)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss1 = criterion(output, y)\n",
    "        loss2 = criterion(aux1, y)\n",
    "        ### TODO:  check how losses are weighted\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "        total=len(y)   \n",
    "        accu = correct / total\n",
    "        self.log(\"train_accuracy\", accu)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        pred = torch.log_softmax(output, dim=1)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "        total=len(y)   \n",
    "        accu = correct / total\n",
    "        self.log(\"val_accuracy\", accu)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.StepLR(optimizer, step_size=2500, gamma=0.5),\n",
    "            'interval': 'step' \n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    \n",
    "cp_path = \"/home/cmf21/pytorch_save/GFRC/Bin/rgb_baseline2_post/patch_model/checkpoint.ckpt.ckpt\"\n",
    "classifier = BinaryWindowClassifier.load_from_checkpoint(checkpoint_path=cp_path)\n",
    "classifier.eval()\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, windows_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.windows_list = windows_list\n",
    "        self.transform = transforms.Compose([Resize((299, 299)),\n",
    "                                             ToTensor(),\n",
    "                                             Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.windows_list[idx]\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "\n",
    "window_dataset = WindowTestDataset(windows_list)\n",
    "windowloader = DataLoader(window_dataset, batch_size=len(window_dataset), shuffle=False)\n",
    "\n",
    "len(windowloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in windowloader:\n",
    "    batch = batch.to(device)\n",
    "    output = classifier(batch)\n",
    "    sm = torch.nn.Softmax(1)\n",
    "    output_sm = sm(output)\n",
    "    pred_prob = output_sm.cpu().detach().numpy()\n",
    "\n",
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of positive windows\n",
    "preds_df = pd.DataFrame(pred_prob, columns=['animal', 'not_animal'])\n",
    "windows_with_preds = pd.concat((windows_whole_im, preds_df), axis=1)\n",
    "windows_filter_out = windows_with_preds[windows_with_preds.animal > 0.5]\n",
    "windows_filter_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_truth(df_in, img_sz):\n",
    "    df_in['xc_pix_rect'] = np.multiply(df_in['xc'], img_sz[1])\n",
    "    df_in['yc_pix_rect'] = np.multiply(df_in['yc'], img_sz[0])\n",
    "    df_in['wid_rect'] = np.multiply(df_in['wid'], img_sz[1])\n",
    "    df_in['hei_rect'] = np.multiply(df_in['height'], img_sz[0])\n",
    "    df_in['xmn'] = np.array(np.subtract(df_in['xc_pix_rect'], np.divide(df_in['wid_rect'], 2)), dtype=np.int)\n",
    "    df_in['xmx'] = np.array(np.add(df_in['xc_pix_rect'], np.divide(df_in['wid_rect'], 2)), dtype=np.int)\n",
    "    df_in['ymn'] = np.array(np.subtract(df_in['yc_pix_rect'], np.divide(df_in['hei_rect'], 2)), dtype=np.int)\n",
    "    df_in['ymx'] = np.array(np.add(df_in['yc_pix_rect'], np.divide(df_in['hei_rect'], 2)), dtype=np.int)\n",
    "    return df_in\n",
    "\n",
    "def windows_out(df_in, tile_w, tile_h, img_w, img_h):\n",
    "    tile_vals = split_locations_array()\n",
    "    xmin = np.subtract(df_in['xc'], np.divide(df_in['wid'], 2))\n",
    "    xmax = np.add(df_in['xc'], np.divide(df_in['wid'], 2))\n",
    "    ymin = np.subtract(df_in['yc'], np.divide(df_in['hei'], 2))\n",
    "    ymax = np.add(df_in['yc'], np.divide(df_in['hei'], 2))\n",
    "    xmin = np.multiply(xmin, tile_w)\n",
    "    xmax = np.multiply(xmax, tile_w)\n",
    "    ymin = np.multiply(ymin, tile_h)\n",
    "    ymax = np.multiply(ymax, tile_h)\n",
    "    tileord = df_in.tile.tolist()\n",
    "    tilerowst = tile_vals[tileord, 0]\n",
    "    tilecolst = tile_vals[tileord, 1]\n",
    "    xmin = np.add(xmin, tilecolst)\n",
    "    xmax = np.add(xmax, tilecolst)\n",
    "    ymin = np.add(ymin, tilerowst)\n",
    "    ymax = np.add(ymax, tilerowst)\n",
    "    xmin = np.maximum(xmin, 0)\n",
    "    xmax = np.minimum(xmax, img_w)\n",
    "    ymin = np.maximum(ymin, 0)\n",
    "    ymax = np.minimum(ymax, img_h)\n",
    "    df_in['xmn'] = np.array(xmin, dtype=np.int)\n",
    "    df_in['xmx'] = np.array(xmax, dtype=np.int)\n",
    "    df_in['ymn'] = np.array(ymin, dtype=np.int)\n",
    "    df_in['ymx'] = np.array(ymax, dtype=np.int)\n",
    "    return df_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare positive windows to trut\n",
    "\n",
    "whole_im_size = [4912, 7360]\n",
    "\n",
    "truths_pixels = windows_truth(truths, whole_im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([img_h, img_w])\n",
    "windows_out_pixels = windows_out(windows_filter_out, img_w, img_h, whole_im_size[1], whole_im_size[0])\n",
    "windows_out_pixels[['xmn', 'xmx', 'ymn', 'ymx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter truths\n",
    "truths_per_im = truths[truths.filename == valid_test_img]\n",
    "truths_per_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_out = ['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal', 'confmat', 'tru_box']\n",
    "results_out = pd.DataFrame(columns=columns_out)\n",
    "\n",
    "if truths_per_im.shape[0] == 0:\n",
    "    results_per_im = windows_out_pixels[['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal']]\n",
    "    results_per_im['confmat'] = 'FP'\n",
    "    results_per_im['tru_box'] = ''\n",
    "    results_out = pd.concat((results_out, results_per_im), axis=0)\n",
    "    \n",
    "#results_out = results_out.reset_index()\n",
    "results_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box1, box2):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xa = max(box1.xmn, box2.xmn)\n",
    "    xb = min(box1.xmx, box2.xmx)\n",
    "    ya = max(box1.ymn, box2.ymn)\n",
    "    yb = min(box1.ymx, box2.ymx)\n",
    "    # compute the area of intersection rectangle\n",
    "    inter_area = max(0, xb - xa + 1) * max(0, yb - ya + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    box1_area = (box1.xmx - box1.xmn + 1) * (box1.ymx - box1.ymn + 1)\n",
    "    box2_area = (box2.xmx - box2.xmn + 1) * (box2.ymx - box2.ymn + 1)\n",
    "    # compute the intersection over union \n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def nms_for_fp(boxes_in, thresh):\n",
    "    \n",
    "    xmins = boxes_in.xmn\n",
    "    xmaxs = boxes_in.xmx\n",
    "    ymins = boxes_in.ymn\n",
    "    ymaxs = boxes_in.ymx\n",
    "    confs = boxes_in.conf\n",
    "    anims = boxes_in.animal\n",
    "    notas = boxes_in.not_animal\n",
    "\n",
    "    boxes_ot = pd.DataFrame(columns=['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal'])\n",
    "\n",
    "    xmins = np.array(xmins.tolist())\n",
    "    xmaxs = np.array(xmaxs.tolist())\n",
    "    ymins = np.array(ymins.tolist())\n",
    "    ymaxs = np.array(ymaxs.tolist())\n",
    "    confs = np.array(confs.tolist())\n",
    "    anims = np.array(anims.tolist())\n",
    "    notas = np.array(notas.tolist())\n",
    "\n",
    "    while len(xmins) > 0:\n",
    "\n",
    "        xmn = xmins[0]\n",
    "        xmns = np.array(xmins[1:])\n",
    "        xmx = xmaxs[0]\n",
    "        xmxs = np.array(xmaxs[1:])\n",
    "        ymn = ymins[0]\n",
    "        ymns = np.array(ymins[1:])\n",
    "        ymx = ymaxs[0]\n",
    "        ymxs = np.array(ymaxs[1:])\n",
    "        cnf = confs[0]\n",
    "        cnfs = np.array(confs[1:])\n",
    "        ani = anims[0]\n",
    "        anis = np.array(anims[1:])\n",
    "        ntz = notas[0]\n",
    "        nots = np.array(notas[1:])\n",
    "\n",
    "        ol_wid = np.minimum(xmx, xmxs) - np.maximum(xmn, xmns)\n",
    "        ol_hei = np.minimum(ymx, ymxs) - np.maximum(ymn, ymns)\n",
    "\n",
    "        ol_x = np.maximum(0, ol_wid)\n",
    "        ol_y = np.maximum(0, ol_hei)\n",
    "\n",
    "        distx = np.subtract(xmxs, xmns)\n",
    "        disty = np.subtract(ymxs, ymns)\n",
    "        bxx = xmx - xmn\n",
    "        bxy = ymx - ymn\n",
    "\n",
    "        ol_area = np.multiply(ol_x, ol_y)\n",
    "        bx_area = bxx * bxy\n",
    "        bxs_area = np.multiply(distx, disty)\n",
    "\n",
    "        ious = np.divide(ol_area, np.subtract(np.add(bxs_area, bx_area), ol_area))\n",
    "        mask_bxs = np.greater(ious, thresh)\n",
    "\n",
    "        if np.sum(mask_bxs) > 0:\n",
    "            box_ot = pd.DataFrame(index=[1], columns=['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal'])\n",
    "            xmns = xmns[mask_bxs]\n",
    "            xmxs = xmxs[mask_bxs]\n",
    "            ymns = ymns[mask_bxs]\n",
    "            ymxs = ymxs[mask_bxs]\n",
    "            cnfs = cnfs[mask_bxs]\n",
    "            anis = anis[mask_bxs]\n",
    "            nots = nots[mask_bxs]\n",
    "\n",
    "            box_ot.xmn.iloc[0] = np.min(xmns)\n",
    "            box_ot.ymn.iloc[0] = np.min(ymns)\n",
    "            box_ot.xmx.iloc[0] = np.max(xmxs)\n",
    "            box_ot.ymx.iloc[0] = np.max(ymxs)\n",
    "            box_ot.conf.iloc[0] = np.mean(cnfs)\n",
    "            box_ot.animal.iloc[0] = np.mean(anis)\n",
    "            box_ot.not_animal.iloc[0] = np.mean(nots)\n",
    "\n",
    "            mask_out = np.repeat(False, len(xmins))\n",
    "            mask_out[0] = True\n",
    "            mask_out[1:] = mask_bxs\n",
    "            mask_out = np.logical_not(mask_out)\n",
    "\n",
    "            xmins = xmins[mask_out]\n",
    "            xmaxs = xmaxs[mask_out]\n",
    "            ymins = ymins[mask_out]\n",
    "            ymaxs = ymaxs[mask_out]\n",
    "            confs = confs[mask_out]\n",
    "            anims = anims[mask_out]\n",
    "            notas = notas[mask_out]\n",
    "        else:\n",
    "            box_ot = pd.DataFrame(index=[1], columns=['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal'])\n",
    "            box_ot.xmn.iloc[0] = xmn\n",
    "            box_ot.ymn.iloc[0] = ymn\n",
    "            box_ot.xmx.iloc[0] = xmx\n",
    "            box_ot.ymx.iloc[0] = ymx\n",
    "            box_ot.conf.iloc[0] = cnf\n",
    "            box_ot.animal.iloc[0] = ani\n",
    "            box_ot.not_animal.iloc[0] = ntz\n",
    "\n",
    "            mask_out = np.repeat(False, len(xmins))\n",
    "            mask_out[0] = True\n",
    "            mask_out = np.logical_not(mask_out)\n",
    "            \n",
    "            xmins = xmins[mask_out]\n",
    "            xmaxs = xmaxs[mask_out]\n",
    "            ymins = ymins[mask_out]\n",
    "            ymaxs = ymaxs[mask_out]\n",
    "            confs = confs[mask_out]\n",
    "            anims = anims[mask_out]\n",
    "            notas = notas[mask_out]\n",
    "        boxes_ot = pd.concat((boxes_ot, box_ot), axis=0)\n",
    "        \n",
    "    boxes_ot['confmat'] = 'FP'\n",
    "    boxes_ot['tru_box'] = ''\n",
    "\n",
    "    return boxes_ot\n",
    " \n",
    "\n",
    "iou_threshold = 0.15\n",
    "\n",
    "def match_truths(windows_out_pixels, truths_per_im):\n",
    "\n",
    "    results_out = pd.DataFrame(columns = ['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal'])\n",
    "\n",
    "    if truths_per_im.shape[0] > 0:\n",
    "        results_per_im = windows_out_pixels[['xmn', 'xmx', 'ymn', 'ymx', 'conf', 'animal', 'not_animal']]\n",
    "        truz = [True] * truths_per_im.shape[0]\n",
    "        rez = [True] * results_per_im.shape[0]\n",
    "        matchz = np.array([True] * results_per_im.shape[0])\n",
    "        for idx, tru in truths_per_im.iterrows():\n",
    "            iouz = []\n",
    "            for res_idx, result in results_per_im.iterrows():\n",
    "                iou = intersection_over_union(tru, result)\n",
    "                iouz.append(iou)\n",
    "            iou_ind = np.argmax(iouz)\n",
    "            if iouz[iou_ind] > iou_threshold:\n",
    "                if rez[iou_ind]: \n",
    "                    best_iou_res = results_per_im.iloc[iou_ind:(iou_ind+1), :]\n",
    "                    best_iou_res['confmat'] = 'TP'\n",
    "                    true_box = f'xmin: {tru.xmn}, xmax:{tru.xmx}, ymin: {tru.ymn}, ymax: {tru.ymx}'\n",
    "                    best_iou_res['tru_box'] = true_box\n",
    "                    results_out = pd.concat((results_out, best_iou_res), axis=0, ignore_index=True)\n",
    "                    truz[idx] = False\n",
    "                    rez[iou_ind] = False\n",
    "            # matchz removes any matches that overlap but are not the most overlapping\n",
    "            match_mask = np.array(iouz) > iou_threshold\n",
    "            matchz[match_mask] = False\n",
    "\n",
    "        results_per_im = results_per_im[matchz]\n",
    "        results_per_im['confmat'] = 'FP'\n",
    "        results_per_im['tru_box'] = ''\n",
    "        results_per_im = nms_for_fp(results_per_im, 0.5)\n",
    "        results_out = pd.concat((results_out, results_per_im), axis=0, ignore_index=True)\n",
    "        false_negatives = truths_per_im[['xmn', 'xmx', 'ymn', 'ymx']]\n",
    "        false_negatives = false_negatives[truz]\n",
    "        false_negatives['conf'] = 0\n",
    "        false_negatives['animal'] = 0\n",
    "        false_negatives['not_animal'] = 0\n",
    "        false_negatives['confmat'] = 'FN'\n",
    "        false_negatives['tru_box'] = ''\n",
    "        results_out = pd.concat((results_out, false_negatives), axis=0, ignore_index=True)\n",
    "\n",
    "    return results_out   \n",
    "\n",
    "results_out = match_truths(windows_out_pixels, truths_per_im)\n",
    "\n",
    "results_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on image\n",
    "import copy\n",
    "\n",
    "whole_im_out = copy.deepcopy(whole_im)\n",
    "\n",
    "# draw TP\n",
    "tp_results = results_out[results_out.confmat == 'TP']\n",
    "for idx, row in tp_results.iterrows():\n",
    "    cv2.rectangle(whole_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(0,255,0),5)\n",
    "    \n",
    "# draw FP\n",
    "fp_results = results_out[results_out.confmat == 'FP']\n",
    "for idx, row in fp_results.iterrows():\n",
    "    cv2.rectangle(whole_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(255,255,0),5)\n",
    "    \n",
    "# draw FN\n",
    "fn_results = results_out[results_out.confmat == 'FN']\n",
    "for idx, row in fn_results.iterrows():\n",
    "    cv2.rectangle(whole_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(255,0,0),5)   \n",
    "    \n",
    "    \n",
    "Image.fromarray(whole_im_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_yolo_pixels = windows_out(windows_whole_im, img_w, img_h, whole_im_size[1], whole_im_size[0])\n",
    "windows_yolo_pixels['animal'] = 0\n",
    "windows_yolo_pixels['not_animal'] = 0\n",
    "yolo_out_wind = match_truths(windows_yolo_pixels, truths_per_im)\n",
    "\n",
    "yolo_out_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_with_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_im_out = copy.deepcopy(whole_im)\n",
    "\n",
    "# draw TP\n",
    "tp_results = yolo_out_wind[yolo_out_wind.confmat == 'TP']\n",
    "for idx, row in tp_results.iterrows():\n",
    "    cv2.rectangle(yolo_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(0,255,0),5)\n",
    "    \n",
    "# draw FP\n",
    "fp_results = yolo_out_wind[yolo_out_wind.confmat == 'FP']\n",
    "for idx, row in fp_results.iterrows():\n",
    "    cv2.rectangle(yolo_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(255,255,0),5)\n",
    "    \n",
    "# draw FN\n",
    "fn_results = yolo_out_wind[yolo_out_wind.confmat == 'FN']\n",
    "for idx, row in fn_results.iterrows():\n",
    "    cv2.rectangle(yolo_im_out,(row.xmn,row.ymn),(row.xmx,row.ymx),(255,0,0),5)   \n",
    "    \n",
    "    \n",
    "Image.fromarray(yolo_im_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-green",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-fifteen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-jacob",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-northern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-coast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-serve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-regular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-privacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-norwegian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-interaction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-western",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-preserve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-leeds",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-junior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-climate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-print",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-revelation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
